{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST 데이터 셋을 이용한 손글씨 인식 Deep Nerual Network\n",
    "\n",
    "아래와 같은 딥러닝의 여러기술을 써서 정확도를 올려보는 실습이다.\n",
    "- ReLU\n",
    "- Dropout\n",
    "- Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Softmax Logistic Regression for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data 구조 조사**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_DATA/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"./MNIST_DATA\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6575454a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[1,0:784].reshape(28,28), cmap=\"gray\", interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Training Error 1.176371993\n",
      "Epoch: 0002 Training Error 0.662472180\n",
      "Epoch: 0003 Training Error 0.550643496\n",
      "Epoch: 0004 Training Error 0.496794565\n",
      "Epoch: 0005 Training Error 0.463764835\n",
      "Epoch: 0006 Training Error 0.440956300\n",
      "Epoch: 0007 Training Error 0.423943600\n",
      "Epoch: 0008 Training Error 0.410669795\n",
      "Epoch: 0009 Training Error 0.399917578\n",
      "Epoch: 0010 Training Error 0.390932974\n",
      "Epoch: 0011 Training Error 0.383309034\n",
      "Epoch: 0012 Training Error 0.376794175\n",
      "Epoch: 0013 Training Error 0.371065391\n",
      "Epoch: 0014 Training Error 0.365894939\n",
      "Epoch: 0015 Training Error 0.361346892\n",
      "Epoch: 0016 Training Error 0.357277366\n",
      "Epoch: 0017 Training Error 0.353523118\n",
      "Epoch: 0018 Training Error 0.350107123\n",
      "Epoch: 0019 Training Error 0.346952044\n",
      "Epoch: 0020 Training Error 0.344147096\n",
      "Epoch: 0021 Training Error 0.341474914\n",
      "Epoch: 0022 Training Error 0.338957943\n",
      "Epoch: 0023 Training Error 0.336639262\n",
      "Epoch: 0024 Training Error 0.334422361\n",
      "Epoch: 0025 Training Error 0.332469314\n",
      "Optimization Finished!\n",
      "Testing Accuracy 0.944999933\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "\n",
    "# tensorflow graph input\n",
    "X = tf.placeholder('float', [None, 784]) # mnist data image of shape 28 * 28 = 784\n",
    "Y = tf.placeholder('float', [None, 10]) # 0-9 digits recognition = > 10 classes\n",
    "\n",
    "# set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Our hypothesis\n",
    "activation = tf.nn.softmax(tf.matmul(X, W) + b)  # Softmax\n",
    "\n",
    "\n",
    "# Cost function: cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(activation), reduction_indices=[1]))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)  # Gradient Descen\n",
    "\n",
    "# Before starting, initialize the variables. We will `run` this first.\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# plot data\n",
    "training_cost = []\n",
    "\n",
    "# Launch the graph,\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        # Fit the line.\n",
    "        for step in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Fit training using batch data\n",
    "\n",
    "            sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "\n",
    "            # Compute average loss\n",
    "            avg_cost += sess.run(cost, feed_dict={X: batch_xs, Y: batch_ys})/total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print (\"Epoch:\", '%04d' %(epoch+1), \"Training Error\", \"{:.9f}\".format(avg_cost))\n",
    "            training_cost.append(avg_cost)\n",
    "    print (\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(activation, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    testing_accuracy = sess.run(accuracy, feed_dict={X: mnist.test.images[:200], Y: mnist.test.labels[:200]})\n",
    "    print(\"Testing Accuracy\",\"{:.9f}\".format(testing_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6572a0ecf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cost function이 Convergence (수렴)하는 것을 그래프로 그려 본다.\n",
    "def plotCost(jvec):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(range(len(jvec)),jvec,'bo')\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Convergence of Cost Function\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Cost function\")\n",
    "\n",
    "plotCost(training_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hidden Layer for MNIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hidden Layer를 1개 추가한다.\n",
    "- Vanishing Gradient 문제를 막기 위해서 ReLU Activation 함수를 사용한다.\n",
    "- 초기 Weight는 `Random Normal`을 사용 한다.\n",
    "    - 주의: 모두 0으로 초기화 하면 Wegiht update가 매번 같게 일어나므로 같은 Layer의 Wegiht들이 모두 같은 값을 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_DATA/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 Training Error 36.593093674\n",
      "Epoch: 0002 Training Error 5.910121407\n",
      "Epoch: 0003 Training Error 2.620937956\n",
      "Epoch: 0004 Training Error 1.630675664\n",
      "Epoch: 0005 Training Error 1.090617501\n",
      "Epoch: 0006 Training Error 0.881382546\n",
      "Epoch: 0007 Training Error 0.655782079\n",
      "Epoch: 0008 Training Error 0.605257626\n",
      "Epoch: 0009 Training Error 0.633176358\n",
      "Epoch: 0010 Training Error 0.511586016\n",
      "Epoch: 0011 Training Error 0.400787890\n",
      "Epoch: 0012 Training Error 0.335924118\n",
      "Epoch: 0013 Training Error 0.319138830\n",
      "Epoch: 0014 Training Error 0.255448152\n",
      "Epoch: 0015 Training Error 0.236012056\n",
      "Optimization Finished!\n",
      "Testing Accuracy 0.97\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./MNIST_DATA\", one_hot=True)\n",
    "# tensorflow graph input\n",
    "X = tf.placeholder('float', [None, 784]) # mnist data image of shape 28 * 28 = 784\n",
    "Y = tf.placeholder('float', [None, 10]) # 0-9 digits recognition = > 10 classes\n",
    "\n",
    "# set model weights\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "\n",
    "B1 = tf.Variable(tf.random_normal([256]))\n",
    "B2 = tf.Variable(tf.random_normal([256]))\n",
    "B3 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# Construct model\n",
    "L1 = tf.nn.relu(tf.add(tf.matmul(X,W1),B1))\n",
    "L2 = tf.nn.relu(tf.add(tf.matmul(L1,W2),B2)) # Hidden layer with RELU activation\n",
    "hypothesis = tf.add(tf.matmul(L2, W3), B3) # No need to use softmax here\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hypothesis, Y)) # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph,\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        # Fit the line.\n",
    "        for step in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # Fit training using batch data\n",
    "\n",
    "            sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "\n",
    "            # Compute average loss\n",
    "            avg_cost += sess.run(cost, feed_dict={X: batch_xs, Y: batch_ys})/total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print (\"Epoch:\", '%04d' %(epoch+1), \"Training Error\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print (\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print (\"Testing Accuracy\", accuracy.eval({X: mnist.test.images[:200], Y: mnist.test.labels[:200]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Weight Initialization\n",
    "- 최종 정확도는 큰 차이는 없지만 초기에 Cost 값이 매우 낮은것을 알 수 있다.\n",
    "- converge 하는 시간을 줄일 수 있으며, 복잡한 Neural Net.을 성공적으로 학습 시킬 수 있는 가능성을 높인다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xavier_initializer(n_inputs, n_outputs, uniform = True):\n",
    "    if uniform:\n",
    "        init_range = tf.sqrt(6.0/ (n_inputs + n_outputs))\n",
    "        return tf.random_uniform_initializer(-init_range, init_range)\n",
    "\n",
    "    else:\n",
    "        stddev = tf.sqrt(3.0 / (n_inputs + n_outputs))\n",
    "        return tf.truncated_normal_initializer(stddev=stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_DATA/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 Training Error 0.200549736\n",
      "Epoch: 0002 Training Error 0.095795567\n",
      "Epoch: 0003 Training Error 0.075184118\n",
      "Epoch: 0004 Training Error 0.063282279\n",
      "Epoch: 0005 Training Error 0.055429216\n",
      "Epoch: 0006 Training Error 0.054017108\n",
      "Epoch: 0007 Training Error 0.051417416\n",
      "Epoch: 0008 Training Error 0.044381755\n",
      "Epoch: 0009 Training Error 0.039349482\n",
      "Epoch: 0010 Training Error 0.042907008\n",
      "Epoch: 0011 Training Error 0.040326667\n",
      "Epoch: 0012 Training Error 0.034777870\n",
      "Epoch: 0013 Training Error 0.037590372\n",
      "Epoch: 0014 Training Error 0.035894262\n",
      "Epoch: 0015 Training Error 0.033769305\n",
      "Optimization Finished!\n",
      "Testing Accuracy 0.98\n"
     ]
    }
   ],
   "source": [
    "def xavier_initializer(n_inputs, n_outputs, uniform = True):\n",
    "    if uniform:\n",
    "        init_range = tf.sqrt(6.0/ (n_inputs + n_outputs))\n",
    "        return tf.random_uniform_initializer(-init_range, init_range)\n",
    "\n",
    "    else:\n",
    "        stddev = tf.sqrt(3.0 / (n_inputs + n_outputs))\n",
    "        return tf.truncated_normal_initializer(stddev=stddev)\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./MNIST_DATA\", one_hot=True)\n",
    "# tensorflow graph input\n",
    "X = tf.placeholder('float', [None, 784]) # mnist data image of shape 28 * 28 = 784\n",
    "Y = tf.placeholder('float', [None, 10]) # 0-9 digits recognition = > 10 classes\n",
    "\n",
    "# set model weights\n",
    "W_1 = tf.get_variable(\"W_1\", shape=[784, 256], initializer=xavier_initializer(784, 256))\n",
    "W_2 = tf.get_variable(\"W_2\", shape=[256, 256], initializer=xavier_initializer(256, 256))\n",
    "W_3 = tf.get_variable(\"W_3\", shape=[256, 10], initializer=xavier_initializer(256, 10))\n",
    "\n",
    "\n",
    "B1 = tf.Variable(tf.zeros([256]))\n",
    "B2 = tf.Variable(tf.zeros([256]))\n",
    "B3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Construct model\n",
    "L1 = tf.nn.relu(tf.add(tf.matmul(X,W_1),B1))\n",
    "L2 = tf.nn.relu(tf.add(tf.matmul(L1,W_2),B2)) # Hidden layer with RELU activation\n",
    "hypothesis = tf.add(tf.matmul(L2, W_3), B3) # No need to use softmax here\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hypothesis, Y)) # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph,\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        # Fit the line.\n",
    "        for step in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # Fit training using batch data\n",
    "\n",
    "            sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "\n",
    "            # Compute average loss\n",
    "            avg_cost += sess.run(cost, feed_dict={X: batch_xs, Y: batch_ys})/total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print (\"Epoch:\", '%04d' %(epoch+1), \"Training Error\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print (\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print (\"Testing Accuracy\", accuracy.eval({X: mnist.test.images[:200], Y: mnist.test.labels[:200]}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Five-Layer Neural Net. with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_DATA/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 Training Error 0.572801369\n",
      "Epoch: 0002 Training Error 0.212843532\n",
      "Epoch: 0003 Training Error 0.157949915\n",
      "Epoch: 0004 Training Error 0.132224367\n",
      "Epoch: 0005 Training Error 0.114936732\n",
      "Epoch: 0006 Training Error 0.104187991\n",
      "Epoch: 0007 Training Error 0.093045380\n",
      "Epoch: 0008 Training Error 0.083886789\n",
      "Epoch: 0009 Training Error 0.081845350\n",
      "Epoch: 0010 Training Error 0.074718143\n",
      "Epoch: 0011 Training Error 0.071650755\n",
      "Epoch: 0012 Training Error 0.063617854\n",
      "Epoch: 0013 Training Error 0.061668005\n",
      "Epoch: 0014 Training Error 0.060254831\n",
      "Epoch: 0015 Training Error 0.055920724\n",
      "Epoch: 0016 Training Error 0.055071080\n",
      "Epoch: 0017 Training Error 0.049940468\n",
      "Epoch: 0018 Training Error 0.049623315\n",
      "Epoch: 0019 Training Error 0.049239787\n",
      "Epoch: 0020 Training Error 0.047966716\n",
      "Epoch: 0021 Training Error 0.048585212\n",
      "Epoch: 0022 Training Error 0.045413566\n",
      "Epoch: 0023 Training Error 0.042845043\n",
      "Epoch: 0024 Training Error 0.040527959\n",
      "Epoch: 0025 Training Error 0.041139369\n",
      "Optimization Finished!\n",
      "Testing Accuracy 0.995\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./MNIST_DATA\", one_hot=True)\n",
    "# tensorflow graph input\n",
    "X = tf.placeholder('float', [None, 784]) # mnist data image of shape 28 * 28 = 784\n",
    "Y = tf.placeholder('float', [None, 10]) # 0-9 digits recognition = > 10 classes\n",
    "\n",
    "# set dropout rate\n",
    "dropout_rate = tf.placeholder(\"float\")\n",
    "\n",
    "# set model weights\n",
    "W__1 = tf.get_variable(\"W__1\", shape=[784, 256], initializer=xavier_initializer(784, 256))\n",
    "W__2 = tf.get_variable(\"W__2\", shape=[256, 256], initializer=xavier_initializer(256, 256))\n",
    "W__3 = tf.get_variable(\"W__3\", shape=[256, 256], initializer=xavier_initializer(256, 256))\n",
    "W__4 = tf.get_variable(\"W__4\", shape=[256, 256], initializer=xavier_initializer(256, 256))\n",
    "W__5 = tf.get_variable(\"W__5\", shape=[256, 10], initializer=xavier_initializer(256, 10))\n",
    "\n",
    "B1 = tf.Variable(tf.random_normal([256]))\n",
    "B2 = tf.Variable(tf.random_normal([256]))\n",
    "B3 = tf.Variable(tf.random_normal([256]))\n",
    "B4 = tf.Variable(tf.random_normal([256]))\n",
    "B5 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# Construct model\n",
    "_L1 = tf.nn.relu(tf.add(tf.matmul(X,W__1),B1))\n",
    "L1 = tf.nn.dropout(_L1, dropout_rate)\n",
    "_L2 = tf.nn.relu(tf.add(tf.matmul(L1, W__2),B2)) # Hidden layer with ReLU activation\n",
    "L2 = tf.nn.dropout(_L2, dropout_rate)\n",
    "_L3 = tf.nn.relu(tf.add(tf.matmul(L2, W__3),B3)) # Hidden layer with ReLU activation\n",
    "L3 = tf.nn.dropout(_L3, dropout_rate)\n",
    "_L4 = tf.nn.relu(tf.add(tf.matmul(L3, W__4),B4)) # Hidden layer with ReLU activation\n",
    "L4 = tf.nn.dropout(_L4, dropout_rate)\n",
    "\n",
    "hypothesis = tf.add(tf.matmul(L4, W__5), B5) # No need to use softmax here\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hypothesis, Y)) # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph,\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        # Fit the line.\n",
    "        for step in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # Fit training using batch data\n",
    "            # set up 0.7 for training time\n",
    "            sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys, dropout_rate: 0.7})\n",
    "\n",
    "            # Compute average loss\n",
    "            avg_cost += sess.run(cost, feed_dict={X: batch_xs, Y: batch_ys, dropout_rate: 0.7})/total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' %(epoch+1), \"Training Error\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print (\"Testing Accuracy\", accuracy.eval({X: mnist.test.images[:200], Y: mnist.test.labels[:200], dropout_rate: 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
