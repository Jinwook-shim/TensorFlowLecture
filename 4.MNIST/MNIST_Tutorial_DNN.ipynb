{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST 데이터 셋을 이용한 손글씨 인식 Deep Nerual Network\n",
    "\n",
    "아래와 같은 딥러닝의 여러기술을 써서 정확도를 올려보는 실습이다.\n",
    "- ReLU\n",
    "- Dropout\n",
    "- Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Softmax Logistic Regression for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data 구조 조사**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_DATA/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"./MNIST_DATA\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0002b6f438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[1,0:784].reshape(28,28), cmap=\"gray\", interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Training Error 1.174406634\n",
      "Epoch: 0002 Training Error 0.662057072\n",
      "Epoch: 0003 Training Error 0.550561634\n",
      "Epoch: 0004 Training Error 0.496714640\n",
      "Epoch: 0005 Training Error 0.463682844\n",
      "Epoch: 0006 Training Error 0.440900077\n",
      "Epoch: 0007 Training Error 0.423956519\n",
      "Epoch: 0008 Training Error 0.410594277\n",
      "Epoch: 0009 Training Error 0.399815456\n",
      "Epoch: 0010 Training Error 0.390898196\n",
      "Epoch: 0011 Training Error 0.383272451\n",
      "Epoch: 0012 Training Error 0.376738432\n",
      "Epoch: 0013 Training Error 0.371002574\n",
      "Epoch: 0014 Training Error 0.365890058\n",
      "Epoch: 0015 Training Error 0.361362095\n",
      "Epoch: 0016 Training Error 0.357240601\n",
      "Epoch: 0017 Training Error 0.353524430\n",
      "Epoch: 0018 Training Error 0.350102182\n",
      "Epoch: 0019 Training Error 0.346980082\n",
      "Epoch: 0020 Training Error 0.344145975\n",
      "Epoch: 0021 Training Error 0.341433940\n",
      "Epoch: 0022 Training Error 0.339001889\n",
      "Epoch: 0023 Training Error 0.336689274\n",
      "Epoch: 0024 Training Error 0.334503716\n",
      "Epoch: 0025 Training Error 0.332440875\n",
      "Optimization Finished!\n",
      "Testing Accuracy 0.944999933\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "\n",
    "# tensorflow graph input\n",
    "X = tf.placeholder('float', [None, 784]) # mnist data image of shape 28 * 28 = 784\n",
    "Y = tf.placeholder('float', [None, 10]) # 0-9 digits recognition = > 10 classes\n",
    "\n",
    "# set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Our hypothesis\n",
    "activation = tf.nn.softmax(tf.matmul(X, W) + b)  # Softmax\n",
    "\n",
    "\n",
    "# Cost function: cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(activation), reduction_indices=[1]))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)  # Gradient Descen\n",
    "\n",
    "# Before starting, initialize the variables. We will `run` this first.\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# plot data\n",
    "training_cost = []\n",
    "\n",
    "# Launch the graph,\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        # Fit the line.\n",
    "        for step in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Fit training using batch data\n",
    "\n",
    "            sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "\n",
    "            # Compute average loss\n",
    "            avg_cost += sess.run(cost, feed_dict={X: batch_xs, Y: batch_ys})/total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print (\"Epoch:\", '%04d' %(epoch+1), \"Training Error\", \"{:.9f}\".format(avg_cost))\n",
    "            training_cost.append(avg_cost)\n",
    "    print (\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(activation, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    testing_accuracy = sess.run(accuracy, feed_dict={X: mnist.test.images[:200], Y: mnist.test.labels[:200]})\n",
    "    print(\"Testing Accuracy\",\"{:.9f}\".format(testing_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0000adcfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cost function이 Convergence (수렴)하는 것을 그래프로 그려 본다.\n",
    "def plotCost(jvec):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(range(len(jvec)),jvec,'bo')\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Convergence of Cost Function\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Cost function\")\n",
    "\n",
    "plotCost(training_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hidden Layer for MNIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hidden Layer를 1개 추가한다.\n",
    "- Vanishing Gradient 문제를 막기 위해서 ReLU Activation 함수를 사용한다.\n",
    "- 초기 Weight는 `Random Normal`을 사용 한다.\n",
    "    - 주의: 모두 0으로 초기화 하면 Wegiht update가 매번 같게 일어나므로 같은 Layer의 Wegiht들이 모두 같은 값을 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_DATA/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 Training Error 36.593093674\n",
      "Epoch: 0002 Training Error 5.910121407\n",
      "Epoch: 0003 Training Error 2.620937956\n",
      "Epoch: 0004 Training Error 1.630675664\n",
      "Epoch: 0005 Training Error 1.090617501\n",
      "Epoch: 0006 Training Error 0.881382546\n",
      "Epoch: 0007 Training Error 0.655782079\n",
      "Epoch: 0008 Training Error 0.605257626\n",
      "Epoch: 0009 Training Error 0.633176358\n",
      "Epoch: 0010 Training Error 0.511586016\n",
      "Epoch: 0011 Training Error 0.400787890\n",
      "Epoch: 0012 Training Error 0.335924118\n",
      "Epoch: 0013 Training Error 0.319138830\n",
      "Epoch: 0014 Training Error 0.255448152\n",
      "Epoch: 0015 Training Error 0.236012056\n",
      "Optimization Finished!\n",
      "Testing Accuracy 0.97\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./MNIST_DATA\", one_hot=True)\n",
    "# tensorflow graph input\n",
    "X = tf.placeholder('float', [None, 784]) # mnist data image of shape 28 * 28 = 784\n",
    "Y = tf.placeholder('float', [None, 10]) # 0-9 digits recognition = > 10 classes\n",
    "\n",
    "# set model weights\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "\n",
    "B1 = tf.Variable(tf.random_normal([256]))\n",
    "B2 = tf.Variable(tf.random_normal([256]))\n",
    "B3 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# Construct model\n",
    "L1 = tf.nn.relu(tf.add(tf.matmul(X,W1),B1))\n",
    "L2 = tf.nn.relu(tf.add(tf.matmul(L1,W2),B2)) # Hidden layer with RELU activation\n",
    "hypothesis = tf.add(tf.matmul(L2, W3), B3) # No need to use softmax here\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hypothesis, Y)) # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph,\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        # Fit the line.\n",
    "        for step in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # Fit training using batch data\n",
    "\n",
    "            sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "\n",
    "            # Compute average loss\n",
    "            avg_cost += sess.run(cost, feed_dict={X: batch_xs, Y: batch_ys})/total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print (\"Epoch:\", '%04d' %(epoch+1), \"Training Error\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print (\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print (\"Testing Accuracy\", accuracy.eval({X: mnist.test.images[:200], Y: mnist.test.labels[:200]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Weight Initialization\n",
    "- 최종 정확도는 큰 차이는 없지만 초기에 Cost 값이 매우 낮은것을 알 수 있다.\n",
    "- converge 하는 시간을 줄일 수 있으며, 복잡한 Neural Net.을 성공적으로 학습 시킬 수 있는 가능성을 높인다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xavier_initializer(n_inputs, n_outputs, uniform = True):\n",
    "    if uniform:\n",
    "        init_range = tf.sqrt(6.0/ (n_inputs + n_outputs))\n",
    "        return tf.random_uniform_initializer(-init_range, init_range)\n",
    "\n",
    "    else:\n",
    "        stddev = tf.sqrt(3.0 / (n_inputs + n_outputs))\n",
    "        return tf.truncated_normal_initializer(stddev=stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_DATA/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 Training Error 0.200549736\n",
      "Epoch: 0002 Training Error 0.095795567\n",
      "Epoch: 0003 Training Error 0.075184118\n",
      "Epoch: 0004 Training Error 0.063282279\n",
      "Epoch: 0005 Training Error 0.055429216\n",
      "Epoch: 0006 Training Error 0.054017108\n",
      "Epoch: 0007 Training Error 0.051417416\n",
      "Epoch: 0008 Training Error 0.044381755\n",
      "Epoch: 0009 Training Error 0.039349482\n",
      "Epoch: 0010 Training Error 0.042907008\n",
      "Epoch: 0011 Training Error 0.040326667\n",
      "Epoch: 0012 Training Error 0.034777870\n",
      "Epoch: 0013 Training Error 0.037590372\n",
      "Epoch: 0014 Training Error 0.035894262\n",
      "Epoch: 0015 Training Error 0.033769305\n",
      "Optimization Finished!\n",
      "Testing Accuracy 0.98\n"
     ]
    }
   ],
   "source": [
    "def xavier_initializer(n_inputs, n_outputs, uniform = True):\n",
    "    if uniform:\n",
    "        init_range = tf.sqrt(6.0/ (n_inputs + n_outputs))\n",
    "        return tf.random_uniform_initializer(-init_range, init_range)\n",
    "\n",
    "    else:\n",
    "        stddev = tf.sqrt(3.0 / (n_inputs + n_outputs))\n",
    "        return tf.truncated_normal_initializer(stddev=stddev)\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./MNIST_DATA\", one_hot=True)\n",
    "# tensorflow graph input\n",
    "X = tf.placeholder('float', [None, 784]) # mnist data image of shape 28 * 28 = 784\n",
    "Y = tf.placeholder('float', [None, 10]) # 0-9 digits recognition = > 10 classes\n",
    "\n",
    "# set model weights\n",
    "W_1 = tf.get_variable(\"W_1\", shape=[784, 256], initializer=xavier_initializer(784, 256))\n",
    "W_2 = tf.get_variable(\"W_2\", shape=[256, 256], initializer=xavier_initializer(256, 256))\n",
    "W_3 = tf.get_variable(\"W_3\", shape=[256, 10], initializer=xavier_initializer(256, 10))\n",
    "\n",
    "\n",
    "B1 = tf.Variable(tf.zeros([256]))\n",
    "B2 = tf.Variable(tf.zeros([256]))\n",
    "B3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Construct model\n",
    "L1 = tf.nn.relu(tf.add(tf.matmul(X,W_1),B1))\n",
    "L2 = tf.nn.relu(tf.add(tf.matmul(L1,W_2),B2)) # Hidden layer with RELU activation\n",
    "hypothesis = tf.add(tf.matmul(L2, W_3), B3) # No need to use softmax here\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hypothesis, Y)) # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph,\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        # Fit the line.\n",
    "        for step in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # Fit training using batch data\n",
    "\n",
    "            sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "\n",
    "            # Compute average loss\n",
    "            avg_cost += sess.run(cost, feed_dict={X: batch_xs, Y: batch_ys})/total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print (\"Epoch:\", '%04d' %(epoch+1), \"Training Error\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print (\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print (\"Testing Accuracy\", accuracy.eval({X: mnist.test.images[:200], Y: mnist.test.labels[:200]}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Five-Layer Neural Net. with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_DATA/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 Training Error 0.572801369\n",
      "Epoch: 0002 Training Error 0.212843532\n",
      "Epoch: 0003 Training Error 0.157949915\n",
      "Epoch: 0004 Training Error 0.132224367\n",
      "Epoch: 0005 Training Error 0.114936732\n",
      "Epoch: 0006 Training Error 0.104187991\n",
      "Epoch: 0007 Training Error 0.093045380\n",
      "Epoch: 0008 Training Error 0.083886789\n",
      "Epoch: 0009 Training Error 0.081845350\n",
      "Epoch: 0010 Training Error 0.074718143\n",
      "Epoch: 0011 Training Error 0.071650755\n",
      "Epoch: 0012 Training Error 0.063617854\n",
      "Epoch: 0013 Training Error 0.061668005\n",
      "Epoch: 0014 Training Error 0.060254831\n",
      "Epoch: 0015 Training Error 0.055920724\n",
      "Epoch: 0016 Training Error 0.055071080\n",
      "Epoch: 0017 Training Error 0.049940468\n",
      "Epoch: 0018 Training Error 0.049623315\n",
      "Epoch: 0019 Training Error 0.049239787\n",
      "Epoch: 0020 Training Error 0.047966716\n",
      "Epoch: 0021 Training Error 0.048585212\n",
      "Epoch: 0022 Training Error 0.045413566\n",
      "Epoch: 0023 Training Error 0.042845043\n",
      "Epoch: 0024 Training Error 0.040527959\n",
      "Epoch: 0025 Training Error 0.041139369\n",
      "Optimization Finished!\n",
      "Testing Accuracy 0.995\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./MNIST_DATA\", one_hot=True)\n",
    "# tensorflow graph input\n",
    "X = tf.placeholder('float', [None, 784]) # mnist data image of shape 28 * 28 = 784\n",
    "Y = tf.placeholder('float', [None, 10]) # 0-9 digits recognition = > 10 classes\n",
    "\n",
    "# set dropout rate\n",
    "dropout_rate = tf.placeholder(\"float\")\n",
    "\n",
    "# set model weights\n",
    "W__1 = tf.get_variable(\"W__1\", shape=[784, 256], initializer=xavier_initializer(784, 256))\n",
    "W__2 = tf.get_variable(\"W__2\", shape=[256, 256], initializer=xavier_initializer(256, 256))\n",
    "W__3 = tf.get_variable(\"W__3\", shape=[256, 256], initializer=xavier_initializer(256, 256))\n",
    "W__4 = tf.get_variable(\"W__4\", shape=[256, 256], initializer=xavier_initializer(256, 256))\n",
    "W__5 = tf.get_variable(\"W__5\", shape=[256, 10], initializer=xavier_initializer(256, 10))\n",
    "\n",
    "B1 = tf.Variable(tf.random_normal([256]))\n",
    "B2 = tf.Variable(tf.random_normal([256]))\n",
    "B3 = tf.Variable(tf.random_normal([256]))\n",
    "B4 = tf.Variable(tf.random_normal([256]))\n",
    "B5 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# Construct model\n",
    "_L1 = tf.nn.relu(tf.add(tf.matmul(X,W__1),B1))\n",
    "L1 = tf.nn.dropout(_L1, dropout_rate)\n",
    "_L2 = tf.nn.relu(tf.add(tf.matmul(L1, W__2),B2)) # Hidden layer with ReLU activation\n",
    "L2 = tf.nn.dropout(_L2, dropout_rate)\n",
    "_L3 = tf.nn.relu(tf.add(tf.matmul(L2, W__3),B3)) # Hidden layer with ReLU activation\n",
    "L3 = tf.nn.dropout(_L3, dropout_rate)\n",
    "_L4 = tf.nn.relu(tf.add(tf.matmul(L3, W__4),B4)) # Hidden layer with ReLU activation\n",
    "L4 = tf.nn.dropout(_L4, dropout_rate)\n",
    "\n",
    "hypothesis = tf.add(tf.matmul(L4, W__5), B5) # No need to use softmax here\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hypothesis, Y)) # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph,\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        # Fit the line.\n",
    "        for step in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # Fit training using batch data\n",
    "            # set up 0.7 for training time\n",
    "            sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys, dropout_rate: 0.7})\n",
    "\n",
    "            # Compute average loss\n",
    "            avg_cost += sess.run(cost, feed_dict={X: batch_xs, Y: batch_ys, dropout_rate: 0.7})/total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' %(epoch+1), \"Training Error\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print (\"Testing Accuracy\", accuracy.eval({X: mnist.test.images[:200], Y: mnist.test.labels[:200], dropout_rate: 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
